## 웹 크롤링 프로세서
- 웹 페이지 다운로드: 해당 웹 페이지의 HTML, CSS, JavaScript 등의 코드를 가져오는 단계
- 페이지 파싱: 다운로드받은 코드를 분석하고 필요한 데이터를 추출하는 단계
- 링크 추출 및 다른 페이지 탐색: 다른 링크를 추출하고 다음 단계로 이동하여 원하는 데이터를 추출하는 단계
- 데이터 추출 및 저장: 분석 및 시각화에 사용하기 위해 데이터를 처리하고 저장하는 단계

### 실습
- `pip install requests beautifulsoup4 selenium`
- `requests`: HTTP 요청을 보내고 응답 받을 수 있는 모듈
- `BeautifulSoup`: HTML 문서에서 원하는 데이터를 추출하는데 사용되는 파이썬 라이브러리
- `Selenium`: 웹 애플리케이션을 테스트하고 자동화하기 위한 파이썬 라이브러리
  - 웹 페이지의 동적인 컨텐츠를 가져오기 위해 사용(검색 결과 등)



> request 모듈은 정적인 부분만 다운로드 가능</br>
정적이다? 서버가 이미 가지고 있는 데이터만

- 동적인 컨텐츠를 다운로드 받을 수 없다! (탕수육이라는 결과를 통해서 변경되는 부분)

그럼?
- 동적인 컨텐츠를 받을 수 있는 방법
- `selenium` : 개발자들이 동적 웹 테스트를 위해서 많이 사용
- 오늘 우리는 크롤링에서 활용
- 마스터할 필요는 없음, 그냥 requests 모듈의 한계점이 있으니까 동적인 컨텐츠는 'selenium'을 통해서 다운로드 받자! 정도